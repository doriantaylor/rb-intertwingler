#+STARTUP: showall hidestars
* desired outcome
  - [ ] command-line tool that can:
    - [ ] spawn a scraper
      - [ ] that traces redirects
        - [ ] that is smart enough to recognize loops
      - [ ] that can either resolve a given list or follow links
      - [ ] that stores content in the content-addressable store
      - [ ] that returns an rdf graph of the metadata
    - [ ] spawn a shell
      - [ ] that can view and edit the rdf graph
        - [ ] with term completion
        - [ ] with shortcuts for certain vocabs
        - [ ] with commands for common bulk rdf operations
    - [ ] spawn a web server
      - [ ] that resolves URIs
        - [ ] that appropriately does redirects
        - [ ] that resolves 410s (gone)
        - [ ] that resolves 300s (multiple choices)
      - [ ] that does content negotiation where applicable
      - [ ] that generates (x)html with all the trimmings
      - [ ] that applies transformation functions to whatever is
        thrown at it (modulo mime type compatibility)
* TODO major refactor
  - [X] methods that are strictly extensions of RDF::Repository
    - [X] actually make these an extension of RDF::Repository
  - [ ] the context that encapsulates configuration data
    - [ ] config parser, coercer, etc
    - [ ] methods that *would* be strict extensions of RDF::Repository
      if not for the fact that they require some of this configuration
  - [-] summary/collection generators
    - [ ] atom feeds
    - [ ] google sitemaps
    - [-] rdf collections of various kinds (temporary)
      - [X] SKOS concept schemes
      - [ ] sioct:ReadingList
      - [ ] sioct:AddressBook
  - [ ] opaque data source providers
    - [ ] file system
      - [ ] git
      - [ ] hg (ugh that will have to
    - [ ] Store::Digest
  - [ ] opaque *Surface* providers
    - [ ] /also/ file system
      - [ ] including rewrite maps
    - [ ] /also/ Store::Digest
    - [ ] something to clean out either one
  - [-] document context
    - [X] markdown to xhtml
    - [-] social media metadata
      - [X] twitter
      - [ ] facebook
      - [ ] google
    - [ ] "modernize"/rewrite legacy tags (xslt? something less/more?)
      - this is sorta done but not organized
    - [ ] inject/repair RDFa
    - [ ] segment compound documents
      - [ ] fragment IDs
    - [X] backlinks
  - [ ] natural language processing (very light)
    - [ ] n-gram extraction
      - [ ] inline markup (dfn/abbr/span, also em/strong etc) treatment
    - [ ] n-gram match (to skos concepts etc)
    - [ ] inline markup generation
* TODO loupe processor <https://vocab.methodandstructure.com/loupe#>
  - [ ] aw yiss
  - [ ] predicate order
  - [ ] predicate show/hide
    - note "hide" can mean invisible but present vs completely omitted
      from the representation
    - gut says "completely omit from representation" should happen at
      the data source level, ie the processor does not have access to
      see what it should be omitting from the representation
  - [ ] value order
  - [ ] value show/hide
  - [ ] label determination
  - [ ] value disposition
    - [ ] resources
      - [ ] link
      - [ ] embed (image, video, audio, iframe, object, script)
      - [ ] inline (fragment)
    - [ ] literals
      - [ ] block
      - [ ] inline
      - [ ] merged
      - [ ] alternates
  - [ ] element selection
    - [ ] block (section, div, paragraph, figure, etc)
    - [ ] list (ol, ul, dl)
      - note rdf:List treatment as well
  - [ ] serialize to (x)html+rdfa
  - [ ] serialize to json-ld (?)
* TODO server
  - [ ] yeeeeahhh gonna need to think about that
  - [ ] path parameter -> transformation function resolver
    - [ ] actually applying the transformations
    - [ ] caching/memoizing the results
      - [ ] some kind of cache clearance and/or denial-of-resource protection
        - only authorized clients get to mint the initial transform?
* TODO command line
  - [ ] depends on command completion, still unresolved
* TODO scraper
  - [ ] i dunno mainly cleanup and reporting i think?
